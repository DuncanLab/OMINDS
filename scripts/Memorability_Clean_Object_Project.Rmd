---
title: "Full_Clean_Object_Project"
author: "Sarah"
date: "08/06/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---

# Load in Data

```{r load packages}

rm(list = ls()) #run this line to clear your environment
library(lme4); library(lmerTest); library(tidyverse); library(data.table); library(dtplyr); library(modes); library(mixtools)

data_dir = "/Users/sarahberger/Desktop/ObjectProject/datafiles/"

load("~/Desktop/ObjectProject/Memorability_Clean_Object_Project.RData")

```

```{r load data and save CSV filename to a new column}
relevantData = list.files(path = data_dir, full.names = TRUE)

data = data.table()
for (i in relevantData) {
  df_temp = tbl_dt(read.csv(i))
  df_temp[, filename := i]
  data = rbind(data, df_temp)
}

data = tbl_dt(data)

```

```{r new columns to denote condition and batch}

data$group = NULL #remove group col

#identify batches 
data[grep(pattern = "batch_1.csv", x = filename, ), batch := 1 ]
data[grep(pattern = "batch_2.csv", x = filename, ), batch := 2 ]
data[grep(pattern = "batch_3.csv", x = filename, ), batch := 3 ]
data[grep(pattern = "batch_4.csv", x = filename, ), batch := 4 ]
data[grep(pattern = "batch_5.csv", x = filename, ), batch := 5 ]
data[grep(pattern = "batch_6.csv", x = filename, ), batch := 6 ]
data[grep(pattern = "batch_7.csv", x = filename, ), batch := 7 ]
data[grep(pattern = "batch_8.csv", x = filename, ), batch := 8 ]
data[grep(pattern = "batch_9.csv", x = filename, ), batch := 9 ]
data[grep(pattern = "batch_10.csv", x = filename, ), batch := 10 ]


#identify conditions
data[grep(pattern = "emotional", x = filename, ), condition := "emotional" ]
data[grep(pattern = "shoebox", x = filename, ), condition := "shoebox" ]
data[grep(pattern = "memory", x = filename, ), condition := "manmade" ]
data[grep(pattern = "outdoors", x = filename, ), condition := "outdoor" ]

```

```{r create new participant IDs and arrange by participant}

to_assign_ids = data[, .N, keyby = .(subject, condition, batch)] 
to_assign_ids[, participant := 1:.N] #add new participant column
to_assign_ids$N = NULL

data = left_join(data, to_assign_ids)

```

```{r arrange and rename variables for convenience, saving full data prior to excluding}

data = data %>% arrange(participant) %>% select(participant, everything()) # arrange by participant

names(data)[8] = "stimulus" # renaming for convenience

# remove unnecessary columns
data$subject = NULL
data$filename = NULL

# rename key presses for retrieval phase to be interpretable
data[blockcode == "retrieval" & response == "36", response := "old"]
data[blockcode == "retrieval" & response == "37", response := "new"]

data_full = copy(data)

```

# Clean Data

## Data cleaning round 1: removing participants who did not complete encoding phase

```{r Exclude participants who did not complete encoding phase}

data[blockcode == "encoding", .N, by = participant][N != 175]

exclude_incomplete_encoding = data[blockcode %in% c("encoding"), .N, by = .(participant, blockcode)][N != 175]$participant

data = data[!participant %in% c(exclude_incomplete_encoding)]

```

## Data cleaning round 2: remove trials that were in the instruction slides

```{r remove responses for images used in instruction slides}

# instruction slides
data = data[
  (condition == "emotional" & ! stimulus %in% c("Dress_Blue.jpg", "Airplane_Commercial.jpg", "Foam_Acoustic.jpg")) |
  (condition == "outdoor" & ! stimulus %in% c("Flower_Sun.jpg", "Airplane_Commercial.jpg", "Oven.jpg")) |
  (condition == "manmade" & ! stimulus %in% c("Flower_Sun.jpg", "Airplane_Commercial.jpg", "Cookie.jpg")) |
  (condition == "shoebox" & ! stimulus %in% c("Flower_Sun.jpg", "Airplane_Commercial.jpg", "Oven.jpg"))
  ]

# white screen
data = data[stimulus != "White_Screen.jpg"]

```

## Data cleaning round 3: excluding abnormal trial latencies

```{r remove recognition trials where rt is abnormally fast (<400ms) or timed out (2000ms)}

data = data[blockcode == "retrieval" & latency %between% c(400, 1999)]

```

## Data cleaning round 4: remove participants who did not complete enough of the retrieval phase

```{r Exclude participants who did not complete enough of the retrieval phase}

exclude_incomplete_retrieval = data[blockcode == "retrieval", .N, by = .(participant)][N < 175]$participant

data = data[!participant %in% c(exclude_incomplete_retrieval)]

```


# Analysis 1: Decide on a participant dprime threshold

## 1.0: Calculate dprime

```{r calculate and visualize dprime}

# this finds what proportion of times the participant got the right answer in new/old trials
stats_by_participant = dcast(subset(data, blockcode=="retrieval"),
                           value.var = "correct",
                           participant ~ trialcode,
                           fill=0,
                           mean, na.rm=T)

names(stats_by_participant)[2:3] = c("FA", "HR") # rename to be more descriptive
stats_by_participant[, FA := 1-FA] # make FA column actually correspond to FA
stats_by_participant[FA==0, FA := 1/175][FA==1, FA := 1-(1/175)] # correct 0s and 1s
stats_by_participant[HR==0, HR := 1/175][HR==1, HR := 1-(1/175)] # correct 0s and 1s

stats_by_participant[, dprime := qnorm(HR) - qnorm(FA)] # calculate dprime

```

```{r Add hits, false alarms, and dprime by participant to the data}

data = merge(data, stats_by_participant, by="participant")

data_unclean = copy(data) # this is before we remove participants with low dprime

```


## 1.1: Look at bimodal distribution

```{r visualize bimodal distributions of dprime}
# the dprime values look bimodal
hist(data$dprime, breaks = 50)

# use the "modes" package to find the antimode (valley in between 2 distributions)
cutoff_dprime = amps(stats_by_participant$dprime)$Antimode[1] # 0.6652773

#visualize density plots
mixmdl= normalmixEM(stats_by_participant$dprime)
plot(mixmdl,which=2, breaks=30)
lines(density(data$dprime), lty=2, lwd=2)
mixmdl$lambda
mixmdl$mu 
mixmdl$sigma 

```


## 1.2 Maximize predictability by removing participants with low dprime

```{r}

dprime_cutoff <- function(DT, cutoff_list){
  
  DT = DT[trialcode == "old_objects"] # we're only looking at HR; only need old objects
  
  # we'll use these later
  len_list = length(cutoff_list)
  num_subs = length(unique(DT$participant))
  
  # create a new table to store the predicting data
  tbl = as.data.frame(matrix(data=NA, ncol=3, nrow=(len_list * num_subs)))
  names(tbl) = c("cutoff", "predsub", "predictability")
  
  i = 1
  
  for(cutoff in cutoff_list){
    
    DT_cutoff = DT[dprime > cutoff] # data for everyone above the current cutoff
    sublist = unique(DT_cutoff$participant) # go through each of these participants
    
    j = 1 # how many subjects are we going through
    
    for(sub in sublist){
      
      test_sub <- DT_cutoff[participant == sub & trialcode == "old_objects", .(participant, stimulus, correct)]
      pred_sub1 <- DT_cutoff[trialcode == "old_objects" & stimulus %in% test_sub$stimulus & participant != sub, .(participant, stimulus, correct)]
      pred_sub_avg_hr <- pred_sub1[, .(HR = mean(correct)), by = stimulus]
      test_pred_data <- left_join(test_sub, pred_sub_avg_hr)
      model <- glm(correct ~ HR, family  = "binomial", data = test_pred_data)
      
      # store info tbl
      index = (num_subs * (i - 1)) + j
      tbl[index, 1] = cutoff
      tbl[index, 2] = sub
      tbl[index, 3] = model$coefficients[2]
      
      j = j + 1 # update which participant we're on
      
    }
    i = i + 1 # update which cutoff value we're on
  }
  return(tbl)
}

antimode = 0.6652773

testlist = c(0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1)

dprime_cutoff_tbl = dprime_cutoff(data_unclean, testlist)

dprime_cutoff_tbl = tbl_dt(dprime_cutoff_tbl)

mean(dprime_cutoff_tbl[cutoff==0]$predictability, na.rm = T) #3.076641
mean(dprime_cutoff_tbl[cutoff==.1]$predictability, na.rm = T) #3.103802
mean(dprime_cutoff_tbl[cutoff==.2]$predictability, na.rm = T) #3.157632
mean(dprime_cutoff_tbl[cutoff==.3]$predictability, na.rm = T) #3.17633
mean(dprime_cutoff_tbl[cutoff==.4]$predictability, na.rm = T) #3.176778
mean(dprime_cutoff_tbl[cutoff==.5]$predictability, na.rm = T) #3.176778
mean(dprime_cutoff_tbl[cutoff==.6]$predictability, na.rm = T) #3.189919
mean(dprime_cutoff_tbl[cutoff==.7]$predictability, na.rm = T) #3.199822
mean(dprime_cutoff_tbl[cutoff==.8]$predictability, na.rm = T) #3.186303
mean(dprime_cutoff_tbl[cutoff==.9]$predictability, na.rm = T) #3.186303
mean(dprime_cutoff_tbl[cutoff==1]$predictability, na.rm = T) #3.184867

names(dprime_cutoff_tbl)[2] = "participant"


```

## 1.3: Remove participants below dprime threshold

```{r exclude participants with dprime below cutoff}

exclude_bimodal_dprime = data[dprime < cutoff_dprime, participant]
length(unique(exclude_bimodal_dprime))

# filter data to eliminate participants
data <- data[!participant %in% c(exclude_bimodal_dprime)]
length(unique(data$participant)) # participants remaining

```

```{r storing clean data}

data_clean = copy(data)

```



# Analysis 2: Simulations to see how many stimulus presentations we need for robust memorability

## 2.1: Create functions for simulations

As we increase the number of "participants" who saw a stimulus, how much does our predictability for memory increase?
```{r Function for simulating how predictable memory is}

run_simulations <- function(tbl, num_participants=50, num_simulations=100){
  
  testsim = as.data.frame(matrix(data=NA, ncol=2, nrow=num_participants*num_simulations))
  names(testsim) = c("num_predicting_participants", "predictability")
  
  for(i in c(1:num_participants)){ 
    
    print(paste("number of participants =", i))
    
    for(j in c(1:num_simulations)){ # number of simulations per # of participants
      
      listsub = sample(unique(tbl$participant)) # randomize list of participants
      testsub = tbl[participant == listsub[1], ][trialcode == "old_objects"] # take old data for one participant
      everyone_else = tbl[!participant == listsub[1],][trialcode == "old_objects"][stimulus %in% testsub$stimulus] # have data for the stimuli testsub saw, without testsub
      
      predsub = group_by(everyone_else, stimulus) %>% # following calc should be by stimulus
          sample_n(i, replace = TRUE) %>% # sample each stimulus i times with replacement
          summarize(HR = mean(correct)) # find the hit rate for each stimulus
      
      mergesub = merge(testsub, predsub, by="stimulus") # this will be stored in "V1"
      model = glm(correct ~ HR.y, family = binomial, mergesub) # this is the glm!
      
      # store the data
      index = (num_simulations * (i-1)) + j
      testsim[index, 1] = i # store the num participants used to predict
      testsim[index, 2] = model$coefficients[2] # store b1 value for this sim
    }
  }
  return(testsim)
}

```


```{r Function for drawing plots that were created by run_simulations}

drawsimplot <- function(tbl){
  tbl %>%
    ggplot(aes(num_predicting_participants, predictability)) +
    # geom_jitter(alpha=0.2) +
    stat_summary(fun.y = mean, geom = "point", size = 3, color = "black") +
    stat_summary(fun.data = mean_se, geom = "errorbar", width = 0, size = 1.5, color = "black") +
    theme_classic() +
    geom_smooth(method="auto") +
    ylim(-1, 3) +
    xlim(0,40)

}

```

##2.2: Run simulations on the full data vs extra batch AND cleaned vs uncleaned data.


```{r Before cleaning, full dataset}

sim_unclean_full = run_simulations(data_unclean) # run the simulation
plt_unclean_full = drawsimplot(sim_unclean_full) # draw the plot

# add extra information to the plot
avgsub_unclean_full = mean(data_unclean[, .(avgsub = length(unique(participant))),
                                                  by=.(stimulus)]$avgsub)
plt_unclean_full + 
  labs(title = "Unclean dataset, all data",
       subtitle = paste("sub per stim =", avgsub_unclean_full)) +
  geom_vline(xintercept = avgsub_unclean_full, col="red")


plt_unclean_full # take a look!

```


```{r Before cleaning, extra dataset}

sim_unclean_extra = run_simulations(data_unclean_sim[batch==5 & condition=="outdoor",])
plt_unclean_extra = drawsimplot(sim_unclean_extra)

avgsub_unclean_extra = mean(data_unclean_sim[batch==5 & condition=="outdoor",
                                            .(avgsub = length(unique(participant))),
                                            by=.(stimulus)]$avgsub)
plt_unclean_extra +
  labs(title = "Uncleaned dataset, extra batch only",
       subtitle = paste("sub per stim =", avgsub_unclean_extra)) +
  geom_vline(xintercept = avgsub_unclean_extra, col = "red")

plt_unclean_extra

```


```{r After cleaning, full dataset}

sim_clean_full = run_simulations(data_clean_sim)
plt_clean_full = drawsimplot(sim_clean_full)

avgsub_clean_full = mean(data_clean_sim[, .(avgsub = length(unique(participant))),
                                       by=.(stimulus)]$avgsub)
plt_clean_full + 
  labs(title = "Clean dataset, all data",
       subtitle = paste("sub per stim =", avgsub_clean_full)) +
  geom_vline(xintercept = avgsub_clean_full, col="red")

plt_clean_full

```


```{r After cleaning, extra dataset}

sim_clean_extra = run_simulations(data_clean_sim[batch==5 & condition=="outdoor",])
plt_clean_extra = drawsimplot(sim_clean_extra)

avgsub_clean_extra = mean(data_clean_sim[batch==5 & condition=="outdoor",
                                          .(avgsub = length(unique(participant))),
                                          by=.(stimulus)]$avgsub)
plt_clean_extra +
  labs(title = "Cleaned dataset, extra batch only") +
  #labs(subtitle = paste("sub per stim =", avgsub_clean_extra)) +
  geom_vline(xintercept = avgsub_clean_extra, col = "red") + 
  xlim=c(0, 40)

plt_clean_extra

```


```{r Organize the plots}

ggpubr::ggarrange(plt_unclean_full, plt_unclean_extra, plt_clean_full, plt_clean_extra,
                  ncol=2, nrow=2)

```

## 2.3: verify that sampling with replacement does not influence beta vales

```{r verify that sampling with replacement does not influence beta vales}

run_simulations_noreplace <- function(tbl, num_participants=50, num_simulations=100){
  
  testsim = as.data.frame(matrix(data=NA, ncol=2, nrow=num_participants*num_simulations))
  names(testsim) = c("num_predicting_participants", "predictability")
  
  for(i in c(1:num_participants)){ 
    
    print(paste("number of participants =", i))
    
    for(j in c(1:num_simulations)){ # number of simulations per # of participants
      
      listsub = sample(unique(tbl$participant)) # randomize list of participants
      testsub = tbl[participant == listsub[1], ][trialcode == "old_objects"] # take old data for one participant
      everyone_else = tbl[!participant == listsub[1],][trialcode == "old_objects"][stimulus %in% testsub$stimulus] # have data for the stimuli testsub saw, without testsub
      
      predsub = group_by(everyone_else, stimulus) %>% # following calc should be by stimulus
          sample_n(i, replace = TRUE) %>% # sample each stimulus i times with replacement
          summarize(HR = mean(correct)) # find the hit rate for each stimulus
      
      mergesub = merge(testsub, predsub, by="stimulus") # this will be stored in "V1"
      model = glm(correct ~ HR.y, family = binomial, mergesub) # this is the glm!
      
      # store the data
      index = (num_simulations * (i-1)) + j
      testsim[index, 1] = i # store the num participants used to predict
      testsim[index, 2] = model$coefficients[2] # store b1 value for this sim
    }
  }
  return(testsim)
}



list = data_unclean[trialcode == "old_objects" & !stimulus %in% c("Dress_Blue.jpg","Airplane_Commercial.jpg","Foam_Acoustic.jpg","Flower_Sun.jpg","Oven.jpg", "Cookie.jpg"), .N, by = stimulus]%>% arrange(N)
list[1,2]

noreplace_sim = run_simulations_noreplace(data_unclean[!stimulus %in% c("Dress_Blue.jpg","Airplane_Commercial.jpg","Foam_Acoustic.jpg","Flower_Sun.jpg","Oven.jpg", "Cookie.jpg")], num_participants = 23, num_simulations = 50)

noreplace_sim_plot = drawsimplot(noreplace_sim) #sampling with replacement does not influence out beta values!
```


# Analysis 3: Which measure is the best measure of memorability

## 3.1: Calculate corrected hit rate (Accounting for participant individuality)

```{r Calculate corrected hit rate}

data_corrected = copy(data_clean)[, avg_mem := correct - HR, by = participant] #subtract a participants average hit rate from whether they got the trial correct or not
#this upweights memorability for those who have poor general memorability (i.e. it's surprising that they got a hir)
data_corrected$onesandzeros = data_corrected$correct #create a new column that stores correct
data_corrected$correct = data_corrected$avg_mem #replace correct column with correct hit rate to be used in calculations


```


## 3.2: running models to test which measure of memorability gives the best predictability for participant hit rate

```{r model to be altered for testing each measure of memorability}

#set parameters

DT = data_clean #change data table from clean -> corrected
participants <- DT[, unique(participant)] 
df <- data.table(participant = participants)

#run loop to predict whether someone will remember a stimulus based on average HR of all participants
for (i in participants) {
  test_sub <- DT[participant == i & trialcode == "old_objects", .(participant, stimulus, correct)] #if doing correct, add onesandzeros
  pred_sub1 <- DT[stimulus %in% test_sub$stimulus & participant != i]
  pred_sub_avg <- stats_by_stim(pred_sub1)
  pred_sub_avg$normHR = scale(pred_sub_avg$HR)
  pred_sub_avg$normdprime = scale(pred_sub_avg$dprime)
  pred_sub_avg$normIES = scale(pred_sub_avg$IES)
  test_pred_data <- left_join(test_sub, pred_sub_avg)
  model2 <- glm(correct ~ normIES, family  = binomial, data = test_pred_data) #if doing corrected change correct to ones and zeros #change stat
  HR_coefficient <- model2$coefficients[2]
  df[participant == i, coef := HR_coefficient]
}
df[, mean(coef)]

#HR_normed = df
mean(HR_normed$coef) #0.3268155

#dprime_normed = df
mean(dprime_normed$coef) #0.2610036

#IES_normed = df
mean(IES_normed$coef) #-0.3014985

#correctedHR_normed = df
mean(correctedHR_normed$coef) #0.3379856

#correcteddprime_normed = df
mean(correcteddprime_normed$coef) #0.1919681


```

## 3.3: Do split sample correlations to see which measure of memorability best correlates with hit rate


```{r split sample correlations}

SS_cor_by_stim_HR <- function(DT, num_sims=10, stat="HR"){
  
  cor_values = as.vector(matrix(data = NA, ncol = 1, nrow = num_sims))

  for (i in 1:num_sims){
    
    scrambled_participants = sample(unique(DT$participant))
    half_participants = length(scrambled_participants) / 2
  
    grp1 = scrambled_participants[1:half_participants]
    grp2 = scrambled_participants[(half_participants + 1) : (half_participants*2)]
    
    grp1_data = stats_by_stim(DT[participant %in% grp1])
    grp2_data = stats_by_stim(DT[participant %in% grp2])
    
    both_grps = left_join(grp1_data, grp2_data, by = "stimulus", suffix = c("_1", "_2"))
    
    cor = cor(both_grps[[paste("HR", "_1", sep="")]], #change HR back to stat to compare the 2 stats with each other
              both_grps[[paste(stat, "_2", sep="")]])
    
    cor_values[i] = cor
  }
  
  hist(cor_values, main = paste("Histogram of", stat))
  print(mean(cor_values, na.rm = TRUE))
  
  return(cor_values)

}

SS_HR_HR = SS_cor_by_stim_HR(data, num_sims = 1000, stat = "HR") # 0.3052638
SS_FA_HR = SS_cor_by_stim_HR(data, num_sims = 1000, stat = "FA") # -0.01882987
SS_dprime_HR = SS_cor_by_stim_HR(data, num_sims = 1000, stat = "dprime") # 0.2157145
SS_rt_HR = SS_cor_by_stim_HR(data, num_sims = 1000, stat = "rt") #-0.150625
SS_IES_HR = SS_cor_by_stim_HR(data, num_sims = 1000, stat = "IES") #-0.2844275
SS_corrected_HR_HR = SS_cor_by_stim_HR(data_corrected, num_sims = 1000, stat = "HR") #0.323796
# this wouldn't run SS_corrected_dprime_HR = SS_cor_by_stim_HR(data_corrected, num_sims = 1000, stat="dprime") #0.2494972
SS_corrected_IES_HR = SS_cor_by_stim_HR(data_corrected, num_sims = 1000, stat = "IES") #0.00626682



```



# Analysis 4: Verify that orientation question does not influence memory

## 4.1: distribution of orientation question amongst poor dprime

```{r distribution of orientation question amongst poor dprime}

low_dprime = data_unclean[dprime < cutoff_dprime]
low_dprime[, length(unique(participant)), by = "condition"] #there are relatively few low dprime participants in manmade (even though it has similar total participants to the other 3 conditions)

data_unclean[, length(unique(participant)), by = "condition"] 

```


## 4.2: Run simulations of predicatibility by orientation question
```{r simulations by orientation question}

manmade_simulation = run_simulations(data_unclean_sim[condition == "manmade"], num_participants = 30, num_simulations = 30)
outdoor_simulation = run_simulations(data_unclean_sim[condition == "outdoor"], num_participants = 30, num_simulations = 30)
shoebox_simulation = run_simulations(data_unclean_sim[condition == "shoebox"], num_participants = 30, num_simulations = 30)
emotional_simulation = run_simulations(data_unclean_sim[condition == "emotional"], num_participants = 30, num_simulations = 30)

plt_manmade = drawsimplot(manmade_simulation) + labs(title = "manmade") +geom_hline(yintercept =1)
plt_outdoor = drawsimplot(outdoor_simulation) + labs(title = "outdoor") +geom_hline(yintercept =1)
plt_shoebox = drawsimplot(shoebox_simulation) + labs(title = "shoebox") +geom_hline(yintercept =1)
plt_emotional = drawsimplot(emotional_simulation) + labs(title = "emotional") +geom_hline(yintercept =1)

ggpubr::ggarrange(plt_manmade, plt_outdoor, plt_shoebox, plt_emotional,
                  ncol=2, nrow=2)

```

## 4.3: Bootstrapping by orientation question

```{r leave one out method for orientation q}

bootstrapping_orientation <- function(tbl, orientation, num_sim = 100, all=F){
  
  testsim = as.data.frame(matrix(data=NA, ncol=1, nrow=num_sim))
  names(testsim) = c(orientation)
  
  if(all==F){tbl_condition = tbl[condition == orientation]}
  else if(all==T){tbl_condition = copy(tbl)}
  
  for(i in 1:num_sim){
    listsub = sample(unique(tbl_condition$participant))
    
    oneout = tbl_condition[participant == listsub[1],]
    everyone_else = tbl_condition[participant != listsub[1],][stimulus %in% oneout$stimulus]
    
    predsub = everyone_else[, .(HRpred = mean(correct)), by=stimulus]

    mergetbl = merge(oneout, predsub, by="stimulus")
    model = glm(correct ~ HRpred, family = "binomial", data=mergetbl)
    
    testsim[i, 1] = model$coefficients[2]
  }
  
  return(testsim)
}

shoebox_boot = bootstrapping_orientation(data_unclean, orientation="shoebox")
emotional_boot = bootstrapping_orientation(data_unclean, orientation= "emotional")
manmade_boot = bootstrapping_orientation(data_unclean, orientation="manmade")
outdoor_boot = bootstrapping_orientation(data_unclean, orientation="outdoor")

all_boot = bootstrapping_orientation(data_unclean, orientation= "all", all=T)

boot_tbl = cbind(shoebox_boot, emotional_boot, manmade_boot, outdoor_boot)
boot_tbl = cbind(boot_tbl, all_boot)

gathered_boot = gather(boot_tbl)

ggplot(gathered_boot, aes(x=key, y=value)) +
  geom_point(position="jitter", aes(colour=key)) +
  stat_summary(fun.y = mean, geom = "point", size=3, color = "black") +
  stat_summary(fun.data = mean_se, geom = "errorbar", width = 0, size = 1.5, color = "black") +
  ylim(-20,20) +
  geom_line(aes(x=key, y=mean(value), colour=key)) +
  geom_hline(yintercept = mean(all_boot$all))


mean(shoebox_boot$shoebox)
mean(all_boot$all)

```

## 4.4: Plot HR by orientation AND batch


```{r Find stats by batch and condition}

# calculate hits and adjust value if 1 or 0
hits_by_batch = data[trialcode == "old_objects",
                               .(memory_hits = mean(correct), number_hits = .N), by = .(stimulus, condition, batch)]
hits_by_batch[memory_hits == 0, memory_hits := 1/(2*(number_hits))]
hits_by_batch[memory_hits == 1, memory_hits := 1 - (1/(2*number_hits))]

# calculate fa and adjust value if 1 or 0
false_alarms_by_batch = data[trialcode == "old_objects",
                                       .(memory_fa = (1-mean(correct)), number_fa = .N), by = .(stimulus, condition, batch)]
false_alarms_by_batch[memory_fa == 0, memory_fa := 1/(2*(number_fa))]
false_alarms_by_batch[memory_fa == 1, memory_fa := 1 - (1/(2*number_fa))]

# calculate dprime
dprime_by_batch = left_join(hits_by_batch, false_alarms_by_batch)
dprime_by_batch[, dprime := qnorm(memory_hits) - qnorm(memory_fa)]

# find average latency
rt_by_batch = data[, .(rt = mean(latency)), by = .(stimulus, condition, batch)]

# find IES
stats_by_batch = left_join(dprime_by_batch, rt_by_batch)
stats_by_batch[, IES := rt / memory_hits]


```

```{r Plot hit rate by batch and condition}

#emotional
emotional_hist <- stats_by_batch[condition == "emotional", ] %>%
  ggplot(aes(x=memory_hits)) + 
  geom_histogram(color="black", fill="white", bins=10) + 
  facet_grid(batch ~ .) +
  coord_cartesian(xlim=c(0,1)) +
  ggtitle("Emotional hits by batch") +
  geom_vline(aes(xintercept=mean(memory_hits), color = "red"), linetype="dashed")

shoebox_hist <- stats_by_batch[condition == "shoebox", ] %>%
  ggplot(aes(x=memory_hits)) + 
  geom_histogram(color="black", fill="white", bins=10) + 
  facet_grid(batch ~ .) +
  coord_cartesian(xlim=c(0,1)) +
  ggtitle("Shoebox hits by batch") +
  geom_vline(aes(xintercept=mean(memory_hits), color = "red"), linetype="dashed")

outdoor_hist <- stats_by_batch[condition == "outdoor", ] %>%
  ggplot(aes(x=memory_hits)) + 
  geom_histogram(color="black", fill="white", bins=10) + 
  facet_grid(batch ~ .) +
  coord_cartesian(xlim=c(0,1)) +
  ggtitle("Outdoor hits by batch") +
  geom_vline(aes(xintercept=mean(memory_hits), color = "red"), linetype="dashed")

manmade_hist <- stats_by_batch[condition == "manmade", ] %>%
  ggplot(aes(x=memory_hits)) + 
  geom_histogram(color="black", fill="white", bins=10) + 
  facet_grid(batch ~ .) +
  coord_cartesian(xlim=c(0,1)) +
  ggtitle("Manmade hits by batch") +
  geom_vline(aes(xintercept=mean(memory_hits), color = "red"), linetype="dashed")

```


# Export data frame that will be used in algorithm

```{r create memorability_dt}

memorability_dt = data_corrected[trialcode == "old_objects", .(memorability = mean(correct)), by = stimulus]

memorability_dt[, scaled_memory_hits := percent_rank(memorability)]

write_csv(memorability_dt, "memorability_dt.csv")
```
